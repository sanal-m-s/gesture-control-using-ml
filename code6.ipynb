{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1ldgCiqk5rEulKz5H0aGX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanal-m-s/gesture-control-using-ml/blob/main/code6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTfgyIDneq-a"
      },
      "outputs": [],
      "source": [
        "from function import *\n",
        "from keras.models import model_from_json\n",
        "from keras.layers import LSTM, Dense\n",
        "from keras.callbacks import TensorBoard\n",
        "import os\n",
        "import time\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "buttonPressed = False\n",
        "buttonCounter = 0\n",
        "buttonDelay = 30\n",
        "\n",
        "json_file = open(\"model.json\", \"r\")\n",
        "model_json = json_file.read()\n",
        "json_file.close()\n",
        "model = model_from_json(model_json)\n",
        "model.load_weights(\"model.h5\")\n",
        "\n",
        "colors = []\n",
        "for i in range(0, 20):\n",
        "    colors.append((245, 117, 16))\n",
        "print(len(colors))\n",
        "\n",
        "# 1. New detection variables\n",
        "sequence = []\n",
        "sentence = []\n",
        "accuracy = []\n",
        "predictions = []\n",
        "threshold = 0.8\n",
        "\n",
        "# Add a variable to keep track of the current index of the displayed image\n",
        "current_image_index = 0\n",
        "\n",
        "# Add variables to track the position of the red circle marker\n",
        "red_circle_position = (0, 0)\n",
        "red_circle_radius = 20\n",
        "\n",
        "# Add variables to track drawing\n",
        "drawing = False\n",
        "prev_point = None\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "# cap = cv2.VideoCapture(\"https://192.168.43.41:8080/video\")\n",
        "# Set mediapipe model\n",
        "with mp_hands.Hands(\n",
        "        model_complexity=0,\n",
        "        min_detection_confidence=0.5,\n",
        "        min_tracking_confidence=0.5) as hands:\n",
        "    # Set the path to the Presentation folder\n",
        "    presentation_folder = \"Presentation\"\n",
        "\n",
        "    while cap.isOpened():\n",
        "\n",
        "        # Read feed\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        # Make detections\n",
        "        cropframe = frame[40:400, 0:300]\n",
        "        frame = cv2.rectangle(frame, (0, 40), (300, 400), 255, 2)\n",
        "        image, results = mediapipe_detection(cropframe, hands)\n",
        "\n",
        "        # 2. Prediction logic\n",
        "        keypoints = extract_keypoints(results)\n",
        "        sequence.append(keypoints)\n",
        "        sequence = sequence[-30:]\n",
        "\n",
        "        try:\n",
        "            if len(sequence) == 30:\n",
        "                res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
        "                predicted_action = actions[np.argmax(res)]\n",
        "                print(predicted_action)\n",
        "                predictions.append(np.argmax(res))\n",
        "\n",
        "                # 3. Viz logic\n",
        "                if np.unique(predictions[-10:])[0] == np.argmax(res):\n",
        "                    if res[np.argmax(res)] > threshold:\n",
        "                        if len(sentence) > 0:\n",
        "                            if actions[np.argmax(res)] != sentence[-1]:\n",
        "                                sentence.append(actions[np.argmax(res)])\n",
        "                                accuracy.append(str(res[np.argmax(res)] * 100))\n",
        "                        else:\n",
        "                            sentence.append(actions[np.argmax(res)])\n",
        "                            accuracy.append(str(res[np.argmax(res)] * 100))\n",
        "\n",
        "                if len(sentence) > 1:\n",
        "                    sentence = sentence[-1:]\n",
        "                    accuracy = accuracy[-1:]\n",
        "\n",
        "                # Viz probabilities\n",
        "                # frame = prob_viz(res, actions, frame, colors, threshold)\n",
        "                # Control images in the Presentation folder based on the recognized gesture\n",
        "                presentation_images = os.listdir(presentation_folder)\n",
        "\n",
        "                if predicted_action == \"A\":\n",
        "                    # Code to navigate to the next image\n",
        "                    current_image_index += 1\n",
        "                    buttonPressed = True\n",
        "                    if current_image_index >= len(presentation_images):\n",
        "                        current_image_index = 0\n",
        "\n",
        "                    if len(presentation_images) > current_image_index:\n",
        "                        image_path = os.path.join(presentation_folder, presentation_images[current_image_index])\n",
        "                        presentation_image = cv2.imread(image_path)\n",
        "                        cv2.imshow('Presentation Image', presentation_image)\n",
        "                        time.sleep(1)\n",
        "\n",
        "                elif predicted_action == \"B\":\n",
        "                    # Code to navigate to the previous image\n",
        "                    current_image_index -= 1\n",
        "                    buttonPressed = True\n",
        "                    if current_image_index < 0:\n",
        "                        current_image_index = len(presentation_images) - 1\n",
        "\n",
        "                    if len(presentation_images) > current_image_index:\n",
        "                        image_path = os.path.join(presentation_folder, presentation_images[current_image_index])\n",
        "                        presentation_image = cv2.imread(image_path)\n",
        "                        cv2.imshow('Presentation Image', presentation_image)\n",
        "                        time.sleep(1)\n",
        "\n",
        "                elif predicted_action == \"C\":\n",
        "                    # Code to track hand landmarks and update the position of the red circle marker\n",
        "                    hand_landmarks = results.multi_hand_landmarks[0] if results.multi_hand_landmarks else None\n",
        "\n",
        "                    if hand_landmarks:\n",
        "                        # Get the coordinates of the tip of the index finger\n",
        "                        index_finger_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
        "                        index_finger_x, index_finger_y = int(index_finger_tip.x * frame.shape[1]), int(\n",
        "                            index_finger_tip.y * frame.shape[0])\n",
        "\n",
        "                        # Update the position of the red circle marker\n",
        "                        red_circle_position = (index_finger_x, index_finger_y)\n",
        "\n",
        "                    # Draw the red circle marker on the current slide\n",
        "                    if len(presentation_images) > current_image_index:\n",
        "                        image_path = os.path.join(presentation_folder, presentation_images[current_image_index])\n",
        "                        presentation_image = cv2.imread(image_path)\n",
        "                        cv2.circle(presentation_image, red_circle_position, red_circle_radius, (0, 0, 255), -1)\n",
        "                        cv2.imshow('Presentation Image', presentation_image)\n",
        "\n",
        "                elif predicted_action == \"D\":\n",
        "                    # Code to draw on the current presentation slide\n",
        "                    hand_landmarks = results.multi_hand_landmarks[0] if results.multi_hand_landmarks else None\n",
        "\n",
        "                    if hand_landmarks:\n",
        "                        index_finger_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
        "                        x, y = int(index_finger_tip.x * frame.shape[1]), int(index_finger_tip.y * frame.shape[0])\n",
        "\n",
        "                        if prev_point is not None:\n",
        "                            cv2.line(presentation_image, prev_point, (x, y), (0, 255, 0), 5)\n",
        "\n",
        "                        prev_point = (x, y)\n",
        "\n",
        "                    # Show the updated presentation image\n",
        "                    cv2.imshow('Presentation Image', presentation_image)\n",
        "\n",
        "                elif predicted_action == \"E\":\n",
        "                    # Code to remove the drawn content on the slide\n",
        "                    presentation_image = cv2.imread(image_path)\n",
        "                    cv2.imshow('Presentation Image', presentation_image)\n",
        "                    drawing_canvas = np.zeros_like(presentation_image)\n",
        "\n",
        "        except Exception as e:\n",
        "            # Handle exceptions\n",
        "            pass\n",
        "\n",
        "            cv2.rectangle(frame, (0, 0), (300, 40), (245, 117, 16), -1)\n",
        "            cv2.putText(frame, \"Output: -\" + ' '.join(sentence) + ''.join(accuracy), (3, 30),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "            # Show to screen\n",
        "            cv2.imshow('OpenCV Feed', frame)\n",
        "\n",
        "            # Break gracefully\n",
        "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
        "                break\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()"
      ]
    }
  ]
}